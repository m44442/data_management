"""
4æ™‚ç‚¹å¯¾å¿œé«˜åº¦çµ±è¨ˆåˆ†æã‚·ã‚¹ãƒ†ãƒ 
çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ2007/2012/2014/2021å¹´ï¼‰ã‚’ç”¨ã„ãŸåŒ…æ‹¬çš„åˆ†æ
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
plt.style.use('default')

class ComprehensiveRetailAnalysis:
    def __init__(self, data_path='data/processed/unified_retail_dataset.json'):
        """åˆæœŸåŒ–"""
        self.data_path = data_path
        self.df = None
        self.results = {}
        
    def load_data(self):
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿"""
        print("ğŸ“Š çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...")
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.df = pd.DataFrame(data)
        
        # ãƒ‡ãƒ¼ã‚¿å‹èª¿æ•´
        self.df['year'] = self.df['year'].astype(int)
        self.df['establishments'] = pd.to_numeric(self.df['establishments'])
        self.df['employees'] = pd.to_numeric(self.df['employees'])
        self.df['sales'] = pd.to_numeric(self.df['sales'], errors='coerce')
        self.df['salesArea'] = pd.to_numeric(self.df['salesArea'], errors='coerce')
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        print(f"ğŸ“… å¯¾è±¡å¹´ä»£: {sorted(self.df['year'].unique())}")
        print(f"ğŸ™ï¸ å¯¾è±¡éƒ½å¸‚: {len(self.df['cityName'].unique())}éƒ½å¸‚")
        
        return self.df
    
    def descriptive_statistics(self):
        """åŸºæœ¬çµ±è¨ˆåˆ†æ"""
        print("\nğŸ” åŸºæœ¬çµ±è¨ˆåˆ†æå®Ÿè¡Œä¸­...")
        
        results = {}
        
        # å…¨ä½“çµ±è¨ˆ
        numeric_cols = ['establishments', 'employees', 'sales', 'salesArea']
        overall_stats = self.df[numeric_cols].describe()
        results['overall_statistics'] = overall_stats.to_dict()
        
        # å¹´åˆ¥çµ±è¨ˆ
        yearly_stats = self.df.groupby('year')[numeric_cols].agg(['mean', 'median', 'std', 'count'])
        results['yearly_statistics'] = {}
        for year in sorted(self.df['year'].unique()):
            year_data = {}
            for col in numeric_cols:
                if col in yearly_stats.columns.get_level_values(0):
                    year_data[col] = {}
                    for stat in ['mean', 'median', 'std', 'count']:
                        if stat in yearly_stats.columns.get_level_values(1):
                            try:
                                value = yearly_stats.loc[year, (col, stat)]
                                if pd.notna(value):
                                    year_data[col][stat] = float(value)
                            except (KeyError, IndexError):
                                pass
            results['yearly_statistics'][str(year)] = year_data
        
        # åœ°åŸŸåˆ¥çµ±è¨ˆ
        regional_stats = self.df.groupby('region')[numeric_cols].agg(['mean', 'median', 'count'])
        results['regional_statistics'] = {}
        for region in self.df['region'].unique():
            if pd.notna(region):
                region_data = {}
                for col in numeric_cols:
                    if col in regional_stats.columns.get_level_values(0):
                        region_data[col] = {}
                        for stat in ['mean', 'median', 'count']:
                            if stat in regional_stats.columns.get_level_values(1):
                                try:
                                    value = regional_stats.loc[region, (col, stat)]
                                    if pd.notna(value):
                                        region_data[col][stat] = float(value)
                                except (KeyError, IndexError):
                                    pass
                results['regional_statistics'][region] = region_data
        
        # éƒ½å¸‚åˆ¥çµ±è¨ˆ
        city_stats = self.df.groupby('cityName')[numeric_cols].agg(['mean', 'median', 'count'])
        results['city_statistics'] = {}
        for city in self.df['cityName'].unique():
            city_data = {}
            for col in numeric_cols:
                if col in city_stats.columns.get_level_values(0):
                    city_data[col] = {}
                    for stat in ['mean', 'median', 'count']:
                        if stat in city_stats.columns.get_level_values(1):
                            try:
                                value = city_stats.loc[city, (col, stat)]
                                if pd.notna(value):
                                    city_data[col][stat] = float(value)
                            except (KeyError, IndexError):
                                pass
            results['city_statistics'][city] = city_data
        
        self.results['descriptive'] = results
        print("âœ… åŸºæœ¬çµ±è¨ˆåˆ†æå®Œäº†")
        return results
    
    def time_series_analysis(self):
        """æ™‚ç³»åˆ—åˆ†æ"""
        print("\nğŸ“ˆ æ™‚ç³»åˆ—åˆ†æå®Ÿè¡Œä¸­...")
        
        results = {}
        
        # å¹´åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰
        yearly_trends = self.df.groupby('year').agg({
            'establishments': ['mean', 'sum'],
            'employees': ['mean', 'sum'],
            'sales': ['mean', 'sum'],
            'salesArea': ['mean', 'sum']
        }).round(2)
        
        # MultiIndexã‚’é©åˆ‡ã«å‡¦ç†
        trends_dict = {}
        for year in yearly_trends.index:
            trends_dict[str(year)] = {}
            for col in ['establishments', 'employees', 'sales', 'salesArea']:
                if col in yearly_trends.columns.get_level_values(0):
                    trends_dict[str(year)][col] = {}
                    for stat in ['mean', 'sum']:
                        if stat in yearly_trends.columns.get_level_values(1):
                            try:
                                value = yearly_trends.loc[year, (col, stat)]
                                if pd.notna(value):
                                    trends_dict[str(year)][col][stat] = float(value)
                            except (KeyError, IndexError):
                                pass
        
        results['yearly_trends'] = trends_dict
        
        # éƒ½å¸‚åˆ¥æ™‚ç³»åˆ—
        city_timeseries = {}
        for city in self.df['cityName'].unique():
            city_data = self.df[self.df['cityName'] == city].sort_values('year')
            if len(city_data) >= 3:  # 3å¹´ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹éƒ½å¸‚ã®ã¿
                # æˆé•·ç‡è¨ˆç®—
                city_trends = {}
                for col in ['establishments', 'employees']:
                    values = city_data[col].values
                    if len(values) > 1:
                        # å¹´å¹³å‡æˆé•·ç‡
                        years = len(values) - 1
                        if values[0] > 0 and values[-1] > 0:
                            cagr = ((values[-1] / values[0]) ** (1/years) - 1) * 100
                            city_trends[f'{col}_cagr'] = round(cagr, 2)
                        
                        # ãƒˆãƒ¬ãƒ³ãƒ‰å‹¾é…
                        x = np.arange(len(values))
                        slope, intercept, r_value, p_value, std_err = stats.linregress(x, values)
                        city_trends[f'{col}_trend_slope'] = round(slope, 2)
                        city_trends[f'{col}_trend_r2'] = round(r_value**2, 3)
                
                city_timeseries[city] = city_trends
        
        results['city_timeseries'] = city_timeseries
        
        self.results['timeseries'] = results
        print("âœ… æ™‚ç³»åˆ—åˆ†æå®Œäº†")
        return results
    
    def regional_comparison(self):
        """åœ°åŸŸæ¯”è¼ƒåˆ†æ"""
        print("\nğŸ—¾ åœ°åŸŸæ¯”è¼ƒåˆ†æå®Ÿè¡Œä¸­...")
        
        results = {}
        
        # åœ°åŸŸåˆ¥å¹³å‡å€¤æ¯”è¼ƒ
        regional_avg = self.df.groupby('region').agg({
            'establishments': 'mean',
            'employees': 'mean',
            'sales': 'mean',
            'salesArea': 'mean'
        }).round(2)
        
        results['regional_averages'] = regional_avg.to_dict()
        
        # åœ°åŸŸé–“ã®æœ‰æ„å·®æ¤œå®šï¼ˆANOVAï¼‰
        regions = self.df['region'].unique()
        if len(regions) > 2:
            anova_results = {}
            for col in ['establishments', 'employees']:
                region_groups = [self.df[self.df['region'] == region][col].dropna().values 
                               for region in regions if len(self.df[self.df['region'] == region]) > 0]
                
                if len(region_groups) > 1 and all(len(group) > 0 for group in region_groups):
                    f_stat, p_value = stats.f_oneway(*region_groups)
                    anova_results[col] = {
                        'f_statistic': round(f_stat, 4),
                        'p_value': round(p_value, 4),
                        'significant': p_value < 0.05
                    }
            
            results['anova_results'] = anova_results
        
        self.results['regional'] = results
        print("âœ… åœ°åŸŸæ¯”è¼ƒåˆ†æå®Œäº†")
        return results
    
    def cluster_analysis(self):
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ"""
        print("\nğŸ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æå®Ÿè¡Œä¸­...")
        
        # éƒ½å¸‚åˆ¥å¹³å‡å€¤ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        city_features = self.df.groupby('cityName').agg({
            'establishments': 'mean',
            'employees': 'mean'
        }).dropna()
        
        if len(city_features) < 3:
            print("âš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}
        
        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(city_features)
        
        # K-means ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆ2-4ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼‰
        cluster_results = {}
        for n_clusters in range(2, min(5, len(city_features))):
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            labels = kmeans.fit_predict(features_scaled)
            
            # ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°
            from sklearn.metrics import silhouette_score
            if len(set(labels)) > 1:
                silhouette_avg = silhouette_score(features_scaled, labels)
                
                cluster_info = {
                    'n_clusters': n_clusters,
                    'silhouette_score': round(silhouette_avg, 3),
                    'cluster_centers': kmeans.cluster_centers_.tolist(),
                    'city_clusters': {}
                }
                
                for i, city in enumerate(city_features.index):
                    cluster_info['city_clusters'][city] = int(labels[i])
                
                cluster_results[f'k_{n_clusters}'] = cluster_info
        
        self.results['clustering'] = cluster_results
        print("âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æå®Œäº†")
        return cluster_results
    
    def correlation_analysis(self):
        """ç›¸é–¢åˆ†æ"""
        print("\nğŸ”— ç›¸é–¢åˆ†æå®Ÿè¡Œä¸­...")
        
        # æ•°å€¤å¤‰æ•°ã®ç›¸é–¢
        numeric_cols = ['establishments', 'employees', 'sales', 'salesArea']
        available_cols = [col for col in numeric_cols if col in self.df.columns]
        
        if len(available_cols) < 2:
            print("âš ï¸ ç›¸é–¢åˆ†æã«ååˆ†ãªæ•°å€¤å¤‰æ•°ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}
        
        corr_matrix = self.df[available_cols].corr()
        
        results = {
            'correlation_matrix': corr_matrix.to_dict(),
            'strong_correlations': {}
        }
        
        # å¼·ã„ç›¸é–¢ï¼ˆ|r| > 0.7ï¼‰ã‚’æŠ½å‡º
        for i in range(len(available_cols)):
            for j in range(i+1, len(available_cols)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:
                    pair = f"{available_cols[i]}_vs_{available_cols[j]}"
                    results['strong_correlations'][pair] = round(corr_val, 3)
        
        self.results['correlation'] = results
        print("âœ… ç›¸é–¢åˆ†æå®Œäº†")
        return results
    
    def efficiency_analysis(self):
        """åŠ¹ç‡æ€§åˆ†æ"""
        print("\nâš¡ åŠ¹ç‡æ€§åˆ†æå®Ÿè¡Œä¸­...")
        
        results = {}
        
        # å¾“æ¥­è€…æ•°/äº‹æ¥­æ‰€æ•° æ¯”ç‡ï¼ˆäº‹æ¥­æ‰€å½“ãŸã‚Šå¾“æ¥­è€…æ•°ï¼‰
        self.df['employees_per_establishment'] = self.df['employees'] / self.df['establishments']
        
        # éƒ½å¸‚åˆ¥åŠ¹ç‡æ€§
        efficiency_stats = self.df.groupby('cityName')['employees_per_establishment'].agg(['mean', 'median', 'std']).round(2)
        
        results['city_efficiency'] = {}
        for city in efficiency_stats.index:
            results['city_efficiency'][city] = {
                'mean': float(efficiency_stats.loc[city, 'mean']) if pd.notna(efficiency_stats.loc[city, 'mean']) else None,
                'median': float(efficiency_stats.loc[city, 'median']) if pd.notna(efficiency_stats.loc[city, 'median']) else None,
                'std': float(efficiency_stats.loc[city, 'std']) if pd.notna(efficiency_stats.loc[city, 'std']) else None
            }
        
        # å¹´åˆ¥åŠ¹ç‡æ€§ãƒˆãƒ¬ãƒ³ãƒ‰
        yearly_efficiency = self.df.groupby('year')['employees_per_establishment'].agg(['mean', 'median', 'std']).round(2)
        results['yearly_efficiency_trends'] = {}
        for year in yearly_efficiency.index:
            results['yearly_efficiency_trends'][str(year)] = {
                'mean': float(yearly_efficiency.loc[year, 'mean']) if pd.notna(yearly_efficiency.loc[year, 'mean']) else None,
                'median': float(yearly_efficiency.loc[year, 'median']) if pd.notna(yearly_efficiency.loc[year, 'median']) else None,
                'std': float(yearly_efficiency.loc[year, 'std']) if pd.notna(yearly_efficiency.loc[year, 'std']) else None
            }
        
        # åœ°åŸŸåˆ¥åŠ¹ç‡æ€§
        regional_efficiency = self.df.groupby('region')['employees_per_establishment'].agg(['mean', 'median', 'std']).round(2)
        results['regional_efficiency'] = {}
        for region in regional_efficiency.index:
            results['regional_efficiency'][region] = {
                'mean': float(regional_efficiency.loc[region, 'mean']) if pd.notna(regional_efficiency.loc[region, 'mean']) else None,
                'median': float(regional_efficiency.loc[region, 'median']) if pd.notna(regional_efficiency.loc[region, 'median']) else None,
                'std': float(regional_efficiency.loc[region, 'std']) if pd.notna(regional_efficiency.loc[region, 'std']) else None
            }
        
        self.results['efficiency'] = results
        print("âœ… åŠ¹ç‡æ€§åˆ†æå®Œäº†")
        return results
    
    def generate_visualizations(self):
        """å¯è¦–åŒ–ç”Ÿæˆ"""
        print("\nğŸ“Š å¯è¦–åŒ–ç”Ÿæˆä¸­...")
        
        import os
        os.makedirs('results/figures', exist_ok=True)
        
        # 1. æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Time Series Analysis - 4-Point Data (2007-2021)', fontsize=16, fontweight='bold')
        
        # å¹´åˆ¥å¹³å‡äº‹æ¥­æ‰€æ•°
        yearly_est = self.df.groupby('year')['establishments'].mean()
        axes[0,0].plot(yearly_est.index, yearly_est.values, marker='o', linewidth=2, markersize=8)
        axes[0,0].set_title('Average Establishments by Year')
        axes[0,0].set_xlabel('Year')
        axes[0,0].set_ylabel('Number of Establishments')
        axes[0,0].grid(True, alpha=0.3)
        
        # å¹´åˆ¥å¹³å‡å¾“æ¥­è€…æ•°
        yearly_emp = self.df.groupby('year')['employees'].mean()
        axes[0,1].plot(yearly_emp.index, yearly_emp.values, marker='s', linewidth=2, markersize=8, color='orange')
        axes[0,1].set_title('Average Employees by Year')
        axes[0,1].set_xlabel('Year')
        axes[0,1].set_ylabel('Number of Employees')
        axes[0,1].grid(True, alpha=0.3)
        
        # éƒ½å¸‚åˆ¥æ¯”è¼ƒï¼ˆ2021å¹´ï¼‰
        latest_data = self.df[self.df['year'] == 2021].sort_values('establishments', ascending=True)
        if len(latest_data) > 0:
            axes[1,0].barh(latest_data['cityName'], latest_data['establishments'])
            axes[1,0].set_title('Establishments by City (2021)')
            axes[1,0].set_xlabel('Number of Establishments')
        
        # åŠ¹ç‡æ€§åˆ†æ
        if 'employees_per_establishment' in self.df.columns:
            efficiency_by_year = self.df.groupby('year')['employees_per_establishment'].mean()
            axes[1,1].plot(efficiency_by_year.index, efficiency_by_year.values, marker='^', linewidth=2, markersize=8, color='green')
            axes[1,1].set_title('Efficiency Trend (Employees per Establishment)')
            axes[1,1].set_xlabel('Year')
            axes[1,1].set_ylabel('Employees/Establishment')
            axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('results/figures/comprehensive_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. åœ°åŸŸæ¯”è¼ƒãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        if len(self.df['region'].unique()) > 1:
            pivot_data = self.df.pivot_table(
                values=['establishments', 'employees'], 
                index='region', 
                columns='year', 
                aggfunc='mean'
            )
            
            if not pivot_data.empty:
                fig, axes = plt.subplots(1, 2, figsize=(16, 6))
                
                # äº‹æ¥­æ‰€æ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
                if 'establishments' in pivot_data.columns.get_level_values(0):
                    est_data = pivot_data['establishments']
                    sns.heatmap(est_data, annot=True, fmt='.0f', cmap='Blues', ax=axes[0])
                    axes[0].set_title('Establishments by Region and Year')
                
                # å¾“æ¥­è€…æ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
                if 'employees' in pivot_data.columns.get_level_values(0):
                    emp_data = pivot_data['employees']
                    sns.heatmap(emp_data, annot=True, fmt='.0f', cmap='Oranges', ax=axes[1])
                    axes[1].set_title('Employees by Region and Year')
                
                plt.tight_layout()
                plt.savefig('results/figures/regional_heatmap.png', dpi=300, bbox_inches='tight')
                plt.close()
        
        print("âœ… å¯è¦–åŒ–å®Œäº†")
        print("ğŸ“ ä¿å­˜å…ˆ: results/figures/")
    
    def save_results(self):
        """åˆ†æçµæœä¿å­˜"""
        print("\nğŸ’¾ åˆ†æçµæœä¿å­˜ä¸­...")
        
        import os
        os.makedirs('results/tables', exist_ok=True)
        
        # JSONå®‰å…¨ãªå½¢å¼ã«å¤‰æ›
        def make_json_safe(obj):
            if isinstance(obj, dict):
                return {k: make_json_safe(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [make_json_safe(v) for v in obj]
            elif isinstance(obj, (np.bool_, bool)):
                return bool(obj)
            elif isinstance(obj, (np.integer, int)):
                return int(obj)
            elif isinstance(obj, (np.floating, float)):
                return float(obj) if not np.isnan(obj) else None
            elif pd.isna(obj):
                return None
            else:
                return obj
        
        safe_results = make_json_safe(self.results)
        
        # JSONå½¢å¼ã§ä¿å­˜
        with open('results/tables/comprehensive_analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(safe_results, f, ensure_ascii=False, indent=2)
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        summary = {
            'analysis_overview': {
                'total_records': int(len(self.df)),
                'cities_analyzed': int(len(self.df['cityName'].unique())),
                'years_covered': [int(year) for year in sorted(self.df['year'].unique())],
                'regions_included': [str(region) for region in self.df['region'].unique() if pd.notna(region)],
                'data_completeness': f"{(len(self.df) / (len(self.df['cityName'].unique()) * len(self.df['year'].unique()))) * 100:.1f}%"
            }
        }
        
        if 'descriptive' in self.results:
            summary['key_findings'] = {
                'avg_establishments': float(round(self.df['establishments'].mean(), 1)),
                'avg_employees': float(round(self.df['employees'].mean(), 1)),
                'efficiency_ratio': float(round(self.df['employees'].sum() / self.df['establishments'].sum(), 2))
            }
        
        with open('results/tables/analysis_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print("âœ… åˆ†æçµæœä¿å­˜å®Œäº†")
        print("ğŸ“ ä¿å­˜å…ˆ:")
        print("  - results/tables/comprehensive_analysis_results.json")
        print("  - results/tables/analysis_summary.json")
    
    def run_full_analysis(self):
        """ãƒ•ãƒ«åˆ†æå®Ÿè¡Œ"""
        print("ğŸš€ 4æ™‚ç‚¹å¯¾å¿œåŒ…æ‹¬çš„åˆ†æã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        print("="*60)
        
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.load_data()
            
            # å„ç¨®åˆ†æå®Ÿè¡Œ
            self.descriptive_statistics()
            self.time_series_analysis()
            self.regional_comparison()
            self.cluster_analysis()
            self.correlation_analysis()
            self.efficiency_analysis()
            
            # å¯è¦–åŒ–ç”Ÿæˆ
            self.generate_visualizations()
            
            # çµæœä¿å­˜
            self.save_results()
            
            print("\n" + "="*60)
            print("ğŸ‰ åŒ…æ‹¬çš„åˆ†æå®Œäº†ï¼")
            print(f"ğŸ“Š åˆ†æå¯¾è±¡: {len(self.df)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
            print(f"ğŸ™ï¸ éƒ½å¸‚æ•°: {len(self.df['cityName'].unique())}")
            print(f"ğŸ“… å¹´æ•°: {len(self.df['year'].unique())}")
            print("ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«:")
            print("  - results/figures/comprehensive_analysis.png")
            print("  - results/figures/regional_heatmap.png") 
            print("  - results/tables/comprehensive_analysis_results.json")
            print("  - results/tables/analysis_summary.json")
            
        except Exception as e:
            print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            raise

if __name__ == "__main__":
    analyzer = ComprehensiveRetailAnalysis()
    analyzer.run_full_analysis()