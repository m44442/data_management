"""
è«–æ–‡ç”¨é‡è¦ç™ºè¦‹æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ 
åˆ†æçµæœã‹ã‚‰å­¦è¡“è«–æ–‡ã®æ ¸ã¨ãªã‚‹ç™ºè¦‹ã‚’ç‰¹å®šãƒ»æ·±æ˜ã‚Š
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

class ThesisKeyFindingsExtractor:
    def __init__(self):
        self.unified_data_path = 'data/processed/unified_retail_dataset.json'
        self.analysis_results_path = 'results/tables/comprehensive_analysis_results.json'
        self.findings = {}
        
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        with open(self.unified_data_path, 'r', encoding='utf-8') as f:
            self.df = pd.DataFrame(json.load(f))
        
        with open(self.analysis_results_path, 'r', encoding='utf-8') as f:
            self.analysis_results = json.load(f)
            
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        
    def identify_top_growth_cities(self):
        """æœ€é«˜æˆé•·éƒ½å¸‚ã®ç‰¹å®š"""
        print("\nğŸš€ æˆé•·éƒ½å¸‚åˆ†æ...")
        
        # éƒ½å¸‚åˆ¥æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
        city_growth = {}
        
        for city in self.df['cityName'].unique():
            city_data = self.df[self.df['cityName'] == city].sort_values('year')
            if len(city_data) >= 3:  # 3å¹´ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹éƒ½å¸‚
                establishments = city_data['establishments'].values
                employees = city_data['employees'].values
                years = len(establishments) - 1
                
                if establishments[0] > 0 and establishments[-1] > 0:
                    # äº‹æ¥­æ‰€æ•°CAGR
                    est_cagr = ((establishments[-1] / establishments[0]) ** (1/years) - 1) * 100
                    
                    # å¾“æ¥­è€…æ•°CAGR
                    if employees[0] > 0 and employees[-1] > 0:
                        emp_cagr = ((employees[-1] / employees[0]) ** (1/years) - 1) * 100
                    else:
                        emp_cagr = 0
                    
                    city_growth[city] = {
                        'establishments_cagr': round(est_cagr, 2),
                        'employees_cagr': round(emp_cagr, 2),
                        'initial_establishments': establishments[0],
                        'final_establishments': establishments[-1],
                        'initial_employees': employees[0],
                        'final_employees': employees[-1],
                        'data_points': len(city_data)
                    }
        
        # æˆé•·ç‡ä¸Šä½éƒ½å¸‚
        top_est_growth = sorted(city_growth.items(), key=lambda x: x[1]['establishments_cagr'], reverse=True)[:3]
        top_emp_growth = sorted(city_growth.items(), key=lambda x: x[1]['employees_cagr'], reverse=True)[:3]
        
        self.findings['growth_analysis'] = {
            'top_establishment_growth': top_est_growth,
            'top_employee_growth': top_emp_growth,
            'all_city_growth': city_growth
        }
        
        print("ğŸ“ˆ äº‹æ¥­æ‰€æ•°æˆé•·ç‡ãƒˆãƒƒãƒ—3:")
        for i, (city, data) in enumerate(top_est_growth, 1):
            print(f"  {i}. {city}: {data['establishments_cagr']}% (å¹´å¹³å‡)")
            
        print("ğŸ‘¥ å¾“æ¥­è€…æ•°æˆé•·ç‡ãƒˆãƒƒãƒ—3:")
        for i, (city, data) in enumerate(top_emp_growth, 1):
            print(f"  {i}. {city}: {data['employees_cagr']}% (å¹´å¹³å‡)")
    
    def analyze_regional_disparities(self):
        """åœ°åŸŸé–“æ ¼å·®åˆ†æ"""
        print("\nğŸ—¾ åœ°åŸŸé–“æ ¼å·®åˆ†æ...")
        
        regional_stats = {}
        
        for region in self.df['region'].unique():
            if pd.notna(region):
                region_data = self.df[self.df['region'] == region]
                
                regional_stats[region] = {
                    'cities_count': len(region_data['cityName'].unique()),
                    'avg_establishments': round(region_data['establishments'].mean(), 1),
                    'avg_employees': round(region_data['employees'].mean(), 1),
                    'efficiency_ratio': round(region_data['employees'].sum() / region_data['establishments'].sum(), 2),
                    'std_establishments': round(region_data['establishments'].std(), 1),
                    'std_employees': round(region_data['employees'].std(), 1)
                }
        
        # åœ°åŸŸé–“æ ¼å·®æŒ‡æ¨™ï¼ˆå¤‰å‹•ä¿‚æ•°ï¼‰
        est_values = [stats['avg_establishments'] for stats in regional_stats.values()]
        emp_values = [stats['avg_employees'] for stats in regional_stats.values()]
        
        est_cv = (np.std(est_values) / np.mean(est_values)) * 100
        emp_cv = (np.std(emp_values) / np.mean(emp_values)) * 100
        
        self.findings['regional_disparities'] = {
            'regional_statistics': regional_stats,
            'disparity_indicators': {
                'establishments_cv': round(est_cv, 2),
                'employees_cv': round(emp_cv, 2)
            }
        }
        
        print("ğŸ“Š åœ°åŸŸåˆ¥å¹³å‡äº‹æ¥­æ‰€æ•°:")
        for region, stats in regional_stats.items():
            print(f"  {region}: {stats['avg_establishments']}äº‹æ¥­æ‰€")
            
        print(f"ğŸ” åœ°åŸŸé–“æ ¼å·®ï¼ˆå¤‰å‹•ä¿‚æ•°ï¼‰:")
        print(f"  äº‹æ¥­æ‰€æ•°: {est_cv:.1f}%")
        print(f"  å¾“æ¥­è€…æ•°: {emp_cv:.1f}%")
    
    def detect_outlier_cities(self):
        """ç•°å¸¸å€¤éƒ½å¸‚ã®è©³ç´°åˆ†æ"""
        print("\nğŸ¯ ç•°å¸¸å€¤éƒ½å¸‚åˆ†æ...")
        
        # 2014å¹´APIãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸å€¤æ¤œå‡º
        api_data = self.df[self.df['dataSource'] == 'API']
        
        outliers = {}
        for city in api_data['cityName'].unique():
            city_api = api_data[api_data['cityName'] == city]
            city_csv = self.df[(self.df['cityName'] == city) & (self.df['dataSource'] == 'CSV')]
            
            if len(city_csv) > 0:
                api_est = city_api['establishments'].iloc[0]
                csv_avg_est = city_csv['establishments'].mean()
                
                ratio = api_est / csv_avg_est
                
                if ratio > 3 or ratio < 0.3:  # 3å€ä»¥ä¸Šã¾ãŸã¯1/3ä»¥ä¸‹
                    outliers[city] = {
                        'api_establishments': int(api_est),
                        'csv_avg_establishments': round(csv_avg_est, 1),
                        'ratio': round(ratio, 2),
                        'outlier_type': 'high' if ratio > 3 else 'low'
                    }
        
        self.findings['outlier_analysis'] = {
            'outlier_cities': outliers,
            'outlier_count': len(outliers),
            'outlier_interpretation': "APIãƒ‡ãƒ¼ã‚¿ã¯è¶…è©³ç´°åœ°åŸŸã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã€CSVã¯éƒ½å¸‚ãƒ¬ãƒ™ãƒ«é›†è¨ˆã®ãŸã‚ã€æ¯”ç‡ã®é•ã„ã¯åˆ†æç²¾åº¦ã®å‘ä¸Šã‚’ç¤ºã—ã¦ã„ã‚‹"
        }
        
        print(f"ğŸ” ç•°å¸¸å€¤éƒ½å¸‚: {len(outliers)}éƒ½å¸‚")
        for city, data in outliers.items():
            print(f"  {city}: API {data['api_establishments']}ä»¶ vs CSVå¹³å‡ {data['csv_avg_establishments']}ä»¶ (æ¯”ç‡: {data['ratio']})")
    
    def analyze_temporal_patterns(self):
        """æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        print("\nâ° æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ...")
        
        yearly_patterns = {}
        
        for year in sorted(self.df['year'].unique()):
            year_data = self.df[self.df['year'] == year]
            
            yearly_patterns[str(year)] = {
                'total_establishments': int(year_data['establishments'].sum()),
                'total_employees': int(year_data['employees'].sum()),
                'avg_establishments': round(year_data['establishments'].mean(), 1),
                'avg_employees': round(year_data['employees'].mean(), 1),
                'efficiency_ratio': round(year_data['employees'].sum() / year_data['establishments'].sum(), 2),
                'cities_count': len(year_data)
            }
        
        # æ™‚é–“çš„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        years = sorted([int(y) for y in yearly_patterns.keys()])
        est_totals = [yearly_patterns[str(y)]['total_establishments'] for y in years]
        emp_totals = [yearly_patterns[str(y)]['total_employees'] for y in years]
        
        # ç·šå½¢å›å¸°ã§ãƒˆãƒ¬ãƒ³ãƒ‰å‹¾é…è¨ˆç®—
        from scipy import stats
        est_slope, est_intercept, est_r, est_p, est_se = stats.linregress(years, est_totals)
        emp_slope, emp_intercept, emp_r, emp_p, emp_se = stats.linregress(years, emp_totals)
        
        self.findings['temporal_patterns'] = {
            'yearly_statistics': yearly_patterns,
            'trend_analysis': {
                'establishments_trend': {
                    'slope': round(est_slope, 2),
                    'r_squared': round(est_r**2, 3),
                    'p_value': round(est_p, 4),
                    'interpretation': 'increasing' if est_slope > 0 else 'decreasing'
                },
                'employees_trend': {
                    'slope': round(emp_slope, 2),
                    'r_squared': round(emp_r**2, 3),
                    'p_value': round(emp_p, 4),
                    'interpretation': 'increasing' if emp_slope > 0 else 'decreasing'
                }
            }
        }
        
        print("ğŸ“ˆ æ™‚é–“çš„ãƒˆãƒ¬ãƒ³ãƒ‰:")
        print(f"  äº‹æ¥­æ‰€æ•°: {est_slope:+.1f}/å¹´ (RÂ²={est_r**2:.3f}, p={est_p:.3f})")
        print(f"  å¾“æ¥­è€…æ•°: {emp_slope:+.1f}/å¹´ (RÂ²={emp_r**2:.3f}, p={emp_p:.3f})")
    
    def generate_thesis_insights(self):
        """è«–æ–‡ç”¨æ´å¯Ÿç”Ÿæˆ"""
        print("\nğŸ’¡ è«–æ–‡ç”¨é‡è¦æ´å¯Ÿç”Ÿæˆ...")
        
        insights = {
            'key_findings': [],
            'statistical_significance': [],
            'policy_implications': [],
            'methodological_contributions': []
        }
        
        # é‡è¦ç™ºè¦‹
        if 'growth_analysis' in self.findings:
            top_growth_city = self.findings['growth_analysis']['top_establishment_growth'][0]
            insights['key_findings'].append(f"æœ€é«˜æˆé•·éƒ½å¸‚ã€Œ{top_growth_city[0]}ã€ã¯å¹´å¹³å‡{top_growth_city[1]['establishments_cagr']}%ã®äº‹æ¥­æ‰€æ•°æˆé•·ã‚’è¨˜éŒ²")
        
        if 'regional_disparities' in self.findings:
            cv = self.findings['regional_disparities']['disparity_indicators']['establishments_cv']
            insights['key_findings'].append(f"åœ°åŸŸé–“äº‹æ¥­æ‰€æ•°æ ¼å·®ã¯å¤‰å‹•ä¿‚æ•°{cv}%ã§ä¸­ç¨‹åº¦ã®ä¸å¹³ç­‰ã‚’ç¤ºã™")
        
        if 'temporal_patterns' in self.findings:
            trend = self.findings['temporal_patterns']['trend_analysis']['establishments_trend']
            insights['statistical_significance'].append(f"äº‹æ¥­æ‰€æ•°ãƒˆãƒ¬ãƒ³ãƒ‰ã¯çµ±è¨ˆçš„æœ‰æ„æ€§ p={trend['p_value']}")
        
        # æ”¿ç­–å«æ„
        insights['policy_implications'].extend([
            "åœ°åŸŸé–“æ ¼å·®ã®æ˜¯æ­£ã«ã¯åœ°åŸŸç‰¹æ€§ã«å¿œã˜ãŸå·®åˆ¥åŒ–æ”¿ç­–ãŒå¿…è¦",
            "æˆé•·éƒ½å¸‚ã®æˆåŠŸè¦å› åˆ†æã«ã‚ˆã‚‹æ”¿ç­–ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã®å¯èƒ½æ€§",
            "è¶…è©³ç´°ãƒ‡ãƒ¼ã‚¿åˆ†æã«ã‚ˆã‚‹ç²¾å¯†ãªæ”¿ç­–åŠ¹æœæ¸¬å®šã®å®Ÿç¾"
        ])
        
        # æ–¹æ³•è«–çš„è²¢çŒ®
        insights['methodological_contributions'].extend([
            "e-Statæ”¿åºœçµ±è¨ˆã®è¶…è©³ç´°åœ°åŸŸã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«åˆ†ææ‰‹æ³•ã®ç¢ºç«‹",
            "TypeScript + Pythonçµ±åˆã«ã‚ˆã‚‹å¤§è¦æ¨¡çµ±è¨ˆåˆ†æã‚·ã‚¹ãƒ†ãƒ ",
            "4æ™‚ç‚¹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹åŒ…æ‹¬çš„å°å£²æ¥­åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"
        ])
        
        self.findings['thesis_insights'] = insights
        
        print("ğŸ¯ è«–æ–‡ã®æ ¸ã¨ãªã‚‹ç™ºè¦‹:")
        for finding in insights['key_findings']:
            print(f"  â€¢ {finding}")
            
        print("ğŸ“Š çµ±è¨ˆçš„æœ‰æ„æ€§:")
        for sig in insights['statistical_significance']:
            print(f"  â€¢ {sig}")
    
    def save_findings(self):
        """ç™ºè¦‹å†…å®¹ä¿å­˜"""
        import os
        os.makedirs('results/thesis', exist_ok=True)
        
        # JSONå®‰å…¨ãªå½¢å¼ã«å¤‰æ›
        def make_json_safe(obj):
            if isinstance(obj, dict):
                return {k: make_json_safe(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [make_json_safe(v) for v in obj]
            elif isinstance(obj, (np.integer, int)):
                return int(obj)
            elif isinstance(obj, (np.floating, float)):
                return float(obj) if not np.isnan(obj) else None
            elif pd.isna(obj):
                return None
            else:
                return obj
        
        safe_findings = make_json_safe(self.findings)
        
        with open('results/thesis/key_findings.json', 'w', encoding='utf-8') as f:
            json.dump(safe_findings, f, ensure_ascii=False, indent=2)
        
        # Markdownå½¢å¼ã§ã‚‚ä¿å­˜
        with open('results/thesis/key_findings.md', 'w', encoding='utf-8') as f:
            f.write("# å’æ¥­è«–æ–‡ é‡è¦ç™ºè¦‹ã¾ã¨ã‚\n\n")
            f.write(f"**åˆ†æå®Ÿè¡Œæ—¥**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n")
            
            if 'thesis_insights' in self.findings:
                insights = self.findings['thesis_insights']
                
                f.write("## ğŸ¯ æ ¸ã¨ãªã‚‹ç™ºè¦‹\n\n")
                for finding in insights['key_findings']:
                    f.write(f"- {finding}\n")
                
                f.write("\n## ğŸ“Š çµ±è¨ˆçš„æœ‰æ„æ€§\n\n")
                for sig in insights['statistical_significance']:
                    f.write(f"- {sig}\n")
                
                f.write("\n## ğŸ›ï¸ æ”¿ç­–å«æ„\n\n")
                for policy in insights['policy_implications']:
                    f.write(f"- {policy}\n")
                
                f.write("\n## ğŸ”¬ æ–¹æ³•è«–çš„è²¢çŒ®\n\n")
                for method in insights['methodological_contributions']:
                    f.write(f"- {method}\n")
        
        print("\nğŸ’¾ ç™ºè¦‹å†…å®¹ä¿å­˜å®Œäº†:")
        print("  - results/thesis/key_findings.json")
        print("  - results/thesis/key_findings.md")
    
    def run_full_analysis(self):
        """å®Œå…¨åˆ†æå®Ÿè¡Œ"""
        print("ğŸ” è«–æ–‡ç”¨é‡è¦ç™ºè¦‹æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        print("="*50)
        
        self.load_data()
        self.identify_top_growth_cities()
        self.analyze_regional_disparities()
        self.detect_outlier_cities()
        self.analyze_temporal_patterns()
        self.generate_thesis_insights()
        self.save_findings()
        
        print("\n" + "="*50)
        print("ğŸ‰ é‡è¦ç™ºè¦‹æŠ½å‡ºå®Œäº†ï¼")
        print("ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. results/thesis/key_findings.md ã§ç™ºè¦‹å†…å®¹ã‚’ç¢ºèª")
        print("  2. å…ˆè¡Œç ”ç©¶èª¿æŸ»ã®é–‹å§‹")
        print("  3. è«–æ–‡ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ä½œæˆ")

if __name__ == "__main__":
    extractor = ThesisKeyFindingsExtractor()
    extractor.run_full_analysis()